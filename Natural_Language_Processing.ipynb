{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Tokenization**\n",
        "\n",
        "Tokenization is the process of breaking down text into smaller pieces, such as words or sentences. This is a fundamental step in NLP, as most text processing requires analyzing words or phrases."
      ],
      "metadata": {
        "id": "xjE8v9b4gFWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Word Tokenization Example\n",
        "```"
      ],
      "metadata": {
        "id": "KOMjF2afgX4J"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Example text\n",
        "text = \"Hello, how are you doing today? I'm learning NLP!\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Tokenized Words:\", tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9ylEDOagtip",
        "outputId": "12c36a13-20fd-42e6-b9b9-2cc21b93eacc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello, how are you doing today? I'm learning NLP!\n",
            "Tokenized Words: ['Hello', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'I', \"'m\", 'learning', 'NLP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Sentence Tokenization Example\n",
        "```"
      ],
      "metadata": {
        "id": "K3ZXYEtdgykR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Example text\n",
        "text = \"NLP is fascinating. It has many real-world applications. I'm excited to learn it!\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Tokenized Sentences:\", sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLpGBIgBgfw6",
        "outputId": "c88ad925-63d5-4952-e08e-4d3e85884149"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: NLP is fascinating. It has many real-world applications. I'm excited to learn it!\n",
            "Tokenized Sentences: ['NLP is fascinating.', 'It has many real-world applications.', \"I'm excited to learn it!\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Stop Words Removal:**\n",
        "\n",
        "Stop words are common words in a language (e.g., \"is,\" \"the,\" \"and\") that don't carry significant meaning for many NLP tasks."
      ],
      "metadata": {
        "id": "DNAFGjXog6In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = \"This is an example sentence showing off the stop words filtration.\"\n",
        "\n",
        "# Tokenize and remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text)\n",
        "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Original Words:\", tokens)\n",
        "print(\"Filtered Words (without stop words):\", filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2RDVwxsg23l",
        "outputId": "c778c6a4-8f4a-42a8-ad5d-5854d754e572"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['This', 'is', 'an', 'example', 'sentence', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "Filtered Words (without stop words): ['example', 'sentence', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Stemming and Lemmatization:**\n",
        "\n",
        "These processes reduce words to their base forms, but in different ways.\n",
        "```\n",
        "Stemming Example\n",
        "```\n",
        "Stemming removes suffixes to bring words to their root form. It can be aggressive and may produce non-standard words"
      ],
      "metadata": {
        "id": "33GV_G86hO0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Example text\n",
        "words = [\"running\", \"runner\", \"ran\", \"easily\", \"fairly\"]\n",
        "\n",
        "# Apply stemming\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Stemmed Words:\", stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFLiOnwqhHSM",
        "outputId": "ccb87c84-6df2-4f6d-a047-5e69cdc77bad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'runner', 'ran', 'easily', 'fairly']\n",
            "Stemmed Words: ['run', 'runner', 'ran', 'easili', 'fairli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Lemmatization Example\n",
        "```\n",
        "Lemmatization uses linguistic rules to return meaningful base forms of words.\n"
      ],
      "metadata": {
        "id": "A6XaxrGPhiSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example text\n",
        "words = [\"running\", \"runner\", \"ran\", \"better\", \"easily\"]\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]  # Specify 'v' for verbs\n",
        "\n",
        "print(\"Original Words:\", words)\n",
        "print(\"Lemmatized Words:\", lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tdoxU3PhfPI",
        "outputId": "6c968c79-f019-46d1-86d5-295883bbc518"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'runner', 'ran', 'better', 'easily']\n",
            "Lemmatized Words: ['run', 'runner', 'run', 'better', 'easily']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Bag of words:**\n",
        "\n",
        "Bag of Words (BoW) is a simple method to represent text as a collection of words, where each word is treated as an individual feature, and the text is represented by the frequency (count) of these words, ignoring grammar, order, and context.\n",
        "```\n",
        "Implementation\n",
        "```"
      ],
      "metadata": {
        "id": "zrT38tUPiL4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Raw data\n",
        "corpus = [\n",
        "    \"I loVe NLP!!! It's AMAZING ðŸ˜Š\",\n",
        "    \"NLP?? Is it fUn, or AMAZING?!!\",\n",
        "    \"Learning NLP and its applications!!!\",\n",
        "    \"NLP is powerful: it solves many real-world problems!\",\n",
        "    \"Do you LOVE NLP? #fun #amazing\"\n",
        "]\n",
        "\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Data cleaning function\n",
        "def clean_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Apply stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Clean the entire corpus\n",
        "cleaned_corpus = [clean_text(doc) for doc in corpus]\n",
        "\n",
        "print(\"Original Corpus:\")\n",
        "print(corpus)\n",
        "print(\"\\nCleaned Corpus:\")\n",
        "print(cleaned_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqxBnTOnhtnN",
        "outputId": "a3cefeb3-77a9-4bdc-d139-4dd200fb08b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Corpus:\n",
            "[\"I loVe NLP!!! It's AMAZING ðŸ˜Š\", 'NLP?? Is it fUn, or AMAZING?!!', 'Learning NLP and its applications!!!', 'NLP is powerful: it solves many real-world problems!', 'Do you LOVE NLP? #fun #amazing']\n",
            "\n",
            "Cleaned Corpus:\n",
            "['love nlp amaz', 'nlp fun amaz', 'learn nlp applic', 'nlp power solv mani realworld problem', 'love nlp fun amaz']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "After cleaning, we convert the cleaned text into a Bag of Words representation.\n",
        "```"
      ],
      "metadata": {
        "id": "KQM4RPFWipIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the BoW model\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(cleaned_corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nBag of Words Representation:\\n\", X_bow.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A179xcgCiij2",
        "outputId": "1c62d2ee-94fc-4e2a-bab5-2755f956413d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['amaz' 'applic' 'fun' 'learn' 'love' 'mani' 'nlp' 'power' 'problem'\n",
            " 'realworld' 'solv']\n",
            "\n",
            "Bag of Words Representation:\n",
            " [[1 0 0 0 1 0 1 0 0 0 0]\n",
            " [1 0 1 0 0 0 1 0 0 0 0]\n",
            " [0 1 0 1 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 1 1 1 1]\n",
            " [1 0 1 0 1 0 1 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.TF-IDF:**\n",
        "\n",
        "TF-IDF provides a more nuanced representation by weighing terms based on their frequency across documents.\n",
        "\n",
        "```\n",
        "Implementation\n",
        "```"
      ],
      "metadata": {
        "id": "0c_AdbWKiwbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create the TF-IDF model\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(cleaned_corpus)\n",
        "\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Representation:\\n\", X_tfidf.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKeZ5Fdhis17",
        "outputId": "0d98021b-4368-4440-bf2e-1264840f4c44"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['amaz' 'applic' 'fun' 'learn' 'love' 'mani' 'nlp' 'power' 'problem'\n",
            " 'realworld' 'solv']\n",
            "\n",
            "TF-IDF Representation:\n",
            " [[0.58148208 0.         0.         0.         0.70050458 0.\n",
            "  0.41372929 0.         0.         0.         0.        ]\n",
            " [0.58148208 0.         0.70050458 0.         0.         0.\n",
            "  0.41372929 0.         0.         0.         0.        ]\n",
            " [0.         0.67009179 0.         0.67009179 0.         0.\n",
            "  0.31930233 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.43739254\n",
            "  0.20841989 0.43739254 0.43739254 0.43739254 0.43739254]\n",
            " [0.47625576 0.         0.57373967 0.         0.57373967 0.\n",
            "  0.33885989 0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis:**\n",
        "```\n",
        "Bag of Words: Focuses on raw counts, which can be useful for simpler models but may not account for word importance.\n",
        "```\n",
        "```\n",
        "TF-IDF: Adds weight to rare words (e.g., \"realworld,\" \"solv\") while down-weighting common words like \"nlp.\n",
        "```"
      ],
      "metadata": {
        "id": "f9od4wrmjHu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-grams\n",
        "\n",
        "## **Definition**\n",
        "An **n-gram** is a contiguous sequence of `n` words from a given text. It captures local word context by considering a fixed number of consecutive words.\n",
        "\n",
        "---\n",
        "\n",
        "## **Types of N-grams**\n",
        "### 1. **Unigram (n=1)**\n",
        "- **Description**: Single words.\n",
        "- **Example**: \"I love NLP\" Unigrams: [\"I\", \"love\", \"NLP\"]\n",
        "\n",
        "### 2. **Bigram (n=2)**\n",
        "- **Description**: Pairs of consecutive words.\n",
        "- **Example**: \"I love NLP\" Bigrams: [(\"I\", \"love\"), (\"love\", \"NLP\")]\n",
        "\n",
        "### 3. **Trigram (n=3)**\n",
        "- **Description**: Three consecutive words.\n",
        "- **Example**: \"I love NLP\" Trigrams: [(\"I\", \"love\", \"NLP\")]\n",
        "```\n",
        "Example Implementation:\n",
        "```"
      ],
      "metadata": {
        "id": "4wTqOfjOkE25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\"I love natural language processing\"]\n",
        "\n",
        "# Generate n-grams (bigrams in this case)\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))  # (2, 2) specifies bigrams\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bigram Representation:\\n\", X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZeAZWB5jD-P",
        "outputId": "a854a08a-742f-4961-e232-eeea6c16e2f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['language processing' 'love natural' 'natural language']\n",
            "Bigram Representation:\n",
            " [[1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## **Use Cases**\n",
        "1. **Text Classification**: Capturing context in sentences for sentiment analysis or spam detection.\n",
        "2. **Machine Translation**: Translating sequences of words instead of individual words.\n",
        "3. **Speech Recognition**: Predicting word sequences for more accurate transcriptions.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "yVTWR6TrkVFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "\n",
        "## **Definition**\n",
        "**Word2Vec** is a neural network-based model that represents words as dense vectors in a continuous vector space. Developed by Google, it captures semantic and syntactic relationships between words.\n",
        "\n",
        "---\n",
        "\n",
        "## **Architectures**\n",
        "1. **CBOW (Continuous Bag of Words)**  \n",
        "   - **Description**: Predicts a word based on its surrounding context words.  \n",
        "   - **Example**:  \n",
        "     For the sentence `\"I love NLP\"`, CBOW predicts `\"love\"` using `\"I\"` and `\"NLP\"` as context.\n",
        "     \n",
        "2. **Skip-gram**  \n",
        "   - **Description**: Predicts the surrounding context words given a single word.  \n",
        "   - **Example**:  \n",
        "     For the word `\"love\"` in `\"I love NLP\"`, Skip-gram predicts `\"I\"` and `\"NLP\"`.\n",
        "\n",
        "---\n",
        "\n",
        "## **How It Works**\n",
        "- Words that appear in similar contexts have similar vector representations in the embedding space.\n",
        "- Example:  \n"
      ],
      "metadata": {
        "id": "MrxBC7LIkiNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example corpus (tokenized sentences)\n",
        "corpus = [\n",
        "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
        "    [\"language\", \"processing\", \"is\", \"fun\"],\n",
        "    [\"I\", \"am\", \"learning\", \"NLP\"]\n",
        "]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences=corpus, vector_size=10, window=2, min_count=1, workers=4)\n",
        "\n",
        "# Access word vector\n",
        "vector = model.wv['language']\n",
        "print(\"Vector for 'language':\", vector)\n",
        "\n",
        "# Find similar words\n",
        "similar = model.wv.most_similar(\"language\")\n",
        "print(\"\\nWords similar to 'language':\", similar)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHnRTZAfkRCy",
        "outputId": "2eb322bd-ee5e-48e9-abca-501a6e74591f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'language': [ 0.07380505 -0.01533471 -0.04536613  0.06554051 -0.0486016  -0.01816018\n",
            "  0.0287658   0.00991874 -0.08285215 -0.09448818]\n",
            "\n",
            "Words similar to 'language': [('processing', 0.5436006188392639), ('I', 0.32937225699424744), ('is', 0.23243840038776398), ('natural', 0.035253241658210754), ('fun', -0.17998705804347992), ('NLP', -0.21133741736412048), ('am', -0.38205230236053467), ('love', -0.5145737528800964), ('learning', -0.5381841063499451)]\n"
          ]
        }
      ]
    }
  ]
}